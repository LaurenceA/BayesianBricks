Reparameterised and non-reparameterised distributions:
  Encode dependencies explicitly using module tree.
  Below a non-reparameterised distribution, everything is the same.
  Above a non-reparameterised distribution, everything is different!
  VI:
    Compute summed lp: easy
  MCMC:
    Go forward from the rv's we're sampling, to find the non-reparameterised distributions which give log-probs
    But we can't go forward, we can only go backwards.
    For each node in tree:
      mark it with the downstream log_probs (including data).  If 

TODOs:
  Reparameterised and non-reparameterised distributions
  Test independent accept/reject steps
  Metropolis: step-size adaptation
  Allow for reduced-rank mass matricies
  Metropolis without having to compute prior ratio:
    Langevin (full differential eq. solution)?
    Single step HMC?
  Docs

  examples:
    neural firing
    calcium? (Exponential + threshold).
    flips in 
    
    
Gamma priors:
  scale: 
    has Gamma conjugate prior
    equals var/mean, and can thus be viewed as a dispersion parameter.
  mean: 
    resonable to use a Gamma prior
  shape:
    obtain using mean / scale

testing


lapses:
  slow, multiplicative drift

noisy accumulator:
  convolution for speed

diffusion to bound (explicit):
  discretise time
  binary variables (stop/don't stop) for each time-point
    these binary variables are observed (give the reaction time)
    use a fairly sharp sigmoid for the top and bottom
    initialize as a straight line from start point to decision

diffusion to bound (integrated):

kalman filter:
  

discrete confidence (e.g. 1-6)
  parameterise thresholds as:
    discrete distribution over 1-6 (but not equal to the marginal dist. over confidences)
    assume a distriubtion (e.g. Gaussian) over confidences (not equal to the marginal dist)
    back solve for the thresholds 



Data:
  Noisy accumulator:
    Valentin Wyart/Alex Pouget
  Diffusion to bound:
    Roozbeh? Angela Yu?
  Kalman Filter:
    Angela Yu?  
  Confidence:
    Bahador?
